{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from time import sleep\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import os\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_site(website):\n",
    "    try:\n",
    "        html = urllib.urlopen(website)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        job_descriptions = soup.findAll('div', class_=\"content\")\n",
    "    except: \n",
    "        pass\n",
    "        clean_site = []\n",
    "    \n",
    "    for job_description in job_descriptions[1:]:\n",
    "        for job in job_description.findAll('li'):\n",
    "            clean_site.append(('').join(job.text.strip().encode('ascii','ignore')))\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = [word for word in clean_site if not word in stop_words]\n",
    "    return \" \".join([word for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page0\n",
      "Page50\n",
      "Page100\n",
      "Page150\n",
      "Page200\n",
      "Page250\n",
      "Page300\n",
      "Page350\n",
      "Page400\n",
      "Page450\n",
      "Page500\n",
      "Page550\n",
      "Page600\n",
      "Page650\n",
      "Page700\n",
      "Page750\n",
      "Page800\n",
      "Page850\n",
      "Page900\n",
      "Page950\n",
      "Page1000\n",
      "Page1050\n",
      "Page1100\n",
      "Page1150\n",
      "Page1200\n",
      "Page1250\n",
      "Page1300\n",
      "Page1350\n",
      "Page1400\n",
      "Page1450\n",
      "Page1500\n",
      "Page1550\n",
      "Page1600\n",
      "Page1650\n",
      "Page1700\n",
      "Page1750\n",
      "Page1800\n",
      "Page1850\n"
     ]
    }
   ],
   "source": [
    "data= []\n",
    "for i in xrange (1899):\n",
    "    filepath = 'data/html_data/linkedin_us_'+ str(i) + '.html'\n",
    "    try:\n",
    "        data.append(cleaning_site(filepath))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    if i%50 == 0:\n",
    "        print 'Page' + str(i) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def describe_results(v_job_desc, nmf_weights, nmf_features, words = 100):\n",
    "    for job_desc_type, skill in enumerate(nmf_features):\n",
    "        print(\"Topic number:%d\" % job_desc_type)\n",
    "        \n",
    "        print(\" \".join([feature_words[i] for i in skill.argsort()[:-words - 1:-1]]))\n",
    "        print '**********************'\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "v_job_desc = vectorizer.fit_transform(data)\n",
    "feature_words = vectorizer.get_feature_names()\n",
    "\n",
    "nmf = NMF(n_components=5)\n",
    "nmf_weights = nmf.fit_transform(v_job_desc)\n",
    "nmf_features = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2]) \n",
    "    return ((tfidf * tfidf.T).A)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number:0\n",
      "data experience analysis business statistical ability develop technical analytics mining analytic skills design solutions complex work product science new tools modeling knowledge techniques large identify years marketing sql teams structured communicate results learning strong problems quantitative research predictive engineering machine models user development methods team sources key understand advanced customer multiple related big unstructured using high performance algorithms expertise use degree projects statistics preferred field analyzing solving able appropriate insights problem analyze trends actionable environment sas working analytical excellent software requirements sets time needs management questions create hadoop communication plus existing varying client computer findings demonstrated lead solve including drive\n",
      "**********************\n",
      "Topic number:1\n",
      "build support customers demonstrate internal new data team evaluate sources predictive analytics insurance preferred hartford shareholders motivate quantitative trust processes characteristics rating glm strive serves partnerships bias operate means agencies excellence regular speed continuously achieve standards execution integrate analytical player exploratory initiatives repeatable validate tasks research cart actuarial qualitative models accuracy outcomes builds integrity plans deliver logistic required availability markets strong metrics quality good improve manipulate skills ensure perform consumer communication provide regression test relationships effective implementation programming methodologies peer drive built review staff high sets analyze software work using analysis solutions knowledge sas underwriting moderate modeling workflows pricing product\n",
      "**********************\n",
      "Topic number:2\n",
      "minimally ideally nlp natural bachelors processing scientist language phd mathematics masters learning computer machine science python statistics experience years data methods java applied languages programming mathematical related using perform physics viewpoint fields classifiers algorithms engineering sentiment storm text scripting pig hadoop background research large operations visualization modeling matlab degree ms amounts proficiency extraction environment advanced optimization deep ruby classification topic plus environments bioinformatics big university domains following mapreduce scikit model college perl graph scale ec2 software content generated clustering presentations filtering sas structures command solr intuitive code hands collaborative diverse mining analyses generate risk paradigms line computational accredited professional historical\n",
      "**********************\n",
      "Topic number:3\n",
      "experience similar data infrastructure redshift spark strong systems distributed working degree hive level algorithms years seniority stardog cs relevance responsibility mongodb like amazon plus knowledge retrieval developing kafka hadoop yarn apache sqs zookeeper oozie hdfs scala big provided file work study os hbase cloud java aws equivalent pipeline bachelor network mapreduce minimum technologies linux designing internals translate s3 including learning troubleshooting scientists maintaining driven based scale information production machine required understanding mathematics statistics implement role field development python industry testing large tools relevant masters ability causal programming skills ms related inference analytics software science languages learn bs building pig nosql\n",
      "**********************\n",
      "Topic number:4\n",
      "models series prediction moving forecasting time building forrest regularization latent dirichlet regressive garch lasso ridge spatial average auto tree experience entrepreneurial spam ensemble bayesian comfortable random write inference solid financial cybercoders send boies brenna following python guru extent resume hear today com room process interested stability format incomplete pto messy missing directly working unlimited love bonus familiarity 401 word vacation decision sense experienced matlab project knowledge growth methods implementation desired frameworks medical make dental manipulating vision ideas environment excel mapreduce spec informal potentially apply skills communication relationship scripts java determine functions managing base consultant scope writing industry developers analysis meet\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "describe_results(v_job_desc, nmf_weights, nmf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
